{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_emb_size = 64\n",
    "emb_size = 256\n",
    "cell_size = emb_size\n",
    "output_size = emb_size\n",
    "word_size = 64\n",
    "words_num = 128\n",
    "read_heads = 4\n",
    "interface_size = word_size * read_heads + word_size * 3 + 5 * read_heads + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epoches = 10000 // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataPath = \"/notebooks/Share/dataset/babi/processed/\"\n",
    "SummariesDir = \"/notebooks/WorkDir/tensorlog\"\n",
    "CheckPtDir = \"./chpts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    'qa1_single-supporting-fact',\n",
    "    'qa20_agents-motivations',\n",
    "    'qa15_basic-deduction',\n",
    "    'qa16_basic-induction',\n",
    "    'qa9_simple-negation',\n",
    "    'qa4_two-arg-relations',\n",
    "    'qa6_yes-no-questions',\n",
    "    'qa10_indefinite-knowledge',\n",
    "    'qa11_basic-coreference',\n",
    "    'qa12_conjunction',\n",
    "    'qa13_compound-coreference',\n",
    "    'qa14_time-reasoning',\n",
    "    'qa17_positional-reasoning',\n",
    "    'qa18_size-reasoning',\n",
    "    'qa19_path-finding',\n",
    "    'qa7_counting',\n",
    "    'qa8_lists-sets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_interface_vector(interface_vector):\n",
    "    \"\"\"\n",
    "        pasres the flat interface_vector into its various components with their\n",
    "        correct shapes\n",
    "        Parameters:\n",
    "        ----------\n",
    "        interface_vector: Tensor (batch_size, interface_vector_size)\n",
    "            the flattened inetrface vector to be parsed\n",
    "        Returns: dict\n",
    "            a dictionary with the components of the interface_vector parsed\n",
    "    \"\"\"\n",
    "\n",
    "    parsed = {}\n",
    "\n",
    "    r_keys_end = word_size * read_heads\n",
    "    r_strengths_end = r_keys_end + read_heads\n",
    "    w_key_end = r_strengths_end + word_size\n",
    "    erase_end = w_key_end + 1 + word_size\n",
    "    write_end = erase_end + word_size\n",
    "    free_end = write_end + read_heads\n",
    "\n",
    "    r_keys_shape = (-1, word_size, read_heads)\n",
    "    r_strengths_shape = (-1,read_heads)\n",
    "    w_key_shape = (-1, word_size, 1)\n",
    "    write_shape = erase_shape = (-1,word_size)\n",
    "    free_shape = (-1, read_heads)\n",
    "    modes_shape = (-1, 3, read_heads)\n",
    "\n",
    "    # parsing the vector into its individual components\n",
    "    parsed['read_keys'] = tf.reshape(interface_vector[:, :r_keys_end], r_keys_shape)\n",
    "    parsed['read_strengths'] = tf.reshape(interface_vector[:, r_keys_end:r_strengths_end], r_strengths_shape)\n",
    "    parsed['write_key'] = tf.reshape(interface_vector[:, r_strengths_end:w_key_end], w_key_shape)\n",
    "    parsed['write_strength'] = tf.reshape(interface_vector[:, w_key_end], (-1, 1))\n",
    "    parsed['erase_vector'] = tf.reshape(interface_vector[:, w_key_end + 1:erase_end], erase_shape)\n",
    "    parsed['write_vector'] = tf.reshape(interface_vector[:, erase_end:write_end], write_shape)\n",
    "    parsed['free_gates'] = tf.reshape(interface_vector[:, write_end:free_end], free_shape)\n",
    "    parsed['allocation_gate'] = tf.expand_dims(interface_vector[:, free_end], 1)\n",
    "    parsed['write_gate'] = tf.expand_dims(interface_vector[:, free_end + 1], 1)\n",
    "    parsed['read_modes'] = tf.reshape(interface_vector[:, free_end + 2:], modes_shape)\n",
    "\n",
    "    # transforming the components to ensure they're in the right ranges\n",
    "    parsed['read_strengths'] = 1 + tf.nn.softplus(parsed['read_strengths'])\n",
    "    parsed['write_strength'] = 1 + tf.nn.softplus(parsed['write_strength'])\n",
    "    parsed['erase_vector'] = tf.nn.sigmoid(parsed['erase_vector'])\n",
    "    parsed['free_gates'] = tf.nn.sigmoid(parsed['free_gates'])\n",
    "    parsed['allocation_gate'] = tf.nn.sigmoid(parsed['allocation_gate'])\n",
    "    parsed['write_gate'] = tf.nn.sigmoid(parsed['write_gate'])\n",
    "    parsed['read_modes'] = tf.nn.softmax(parsed['read_modes'], 1)\n",
    "\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def __input_stage__(op, last_read_vectors):\n",
    "    \"\"\"\n",
    "        processes input data through the controller network and returns the\n",
    "        pre-output and interface_vector\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: Tensor (batch_size, input_size)\n",
    "            the input data batch\n",
    "        last_read_vectors: (batch_size, word_size, read_heads)\n",
    "            the last batch of read vectors from memory\n",
    "        state: Tuple\n",
    "            state vectors if the network is recurrent\n",
    "        Returns: Tuple\n",
    "            pre-output: Tensor (batch_size, output_size)\n",
    "            parsed_interface_vector: dict\n",
    "    \"\"\"\n",
    "\n",
    "    flat_read_vectors = tf.reshape(\n",
    "        last_read_vectors,\n",
    "        (-1, word_size * read_heads))\n",
    "    cell_input = tf.concat(1, [op, flat_read_vectors])\n",
    "    \n",
    "    input_weights = tf.get_variable(\n",
    "        \"input_weights\", [word_size * read_heads + emb_size, cell_size])\n",
    " \n",
    "    return tf.matmul(cell_input, input_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __rnn_stage__(cell_input, state, cell):\n",
    "    return cell(cell_input, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __interface_stage__(cell_output):\n",
    "    interface_weights = tf.get_variable(\n",
    "        \"interface_weights\", [cell_size, interface_size])\n",
    "    \n",
    "    interface = tf.matmul(cell_output, interface_weights)\n",
    "   \n",
    "    return parse_interface_vector(interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __output_stage__(cell_output, new_read):\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [cell_size, output_size])\n",
    "    \n",
    "    mem_output_weights = tf.get_variable(\n",
    "        \"mem_output_weights\", [word_size * read_heads, output_size])\n",
    "    \n",
    "    flat_read = tf.reshape(new_read, (-1, word_size * read_heads))\n",
    "    \n",
    "    controller_output = tf.matmul(cell_output, output_weights)\n",
    "    memory_output = tf.matmul(flat_read, mem_output_weights)\n",
    "    return controller_output + memory_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def DNC_controller_exe_query(cur_inp, last_state, last_reads, cell):\n",
    "    \n",
    "    with tf.variable_scope(\"controller_input\"):\n",
    "        cell_input = __input_stage__(cur_inp, last_reads)\n",
    "        \n",
    "    with tf.variable_scope(\"controller_cell\"):\n",
    "        cell_output, cell_state = __rnn_stage__(cell_input, last_state, cell)\n",
    "        \n",
    "    with tf.variable_scope(\"controller_interface\"):\n",
    "        interface_dict = __interface_stage__(cell_output)\n",
    "        \n",
    "    return cell_output, cell_state, interface_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DNC_controller_wb_output(cell_output, mem_reads):\n",
    "    with tf.variable_scope(\"controller_output\"):\n",
    "        DNC_output = __output_stage__(cell_output, mem_reads)\n",
    "        \n",
    "    return DNC_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __get_memory__():\n",
    "    with tf.variable_scope(\"memory_banks\"):\n",
    "        return (\n",
    "            tf.get_variable(\"memory_matrix\", [batch_size, words_num, word_size],\n",
    "                initializer=tf.constant_initializer(1e-6), trainable=False),\n",
    "            tf.get_variable(\"usage_vector\", [batch_size, words_num],\n",
    "                initializer=tf.constant_initializer(0), trainable=False),\n",
    "            tf.get_variable(\"precedence_vector\", [batch_size, words_num],\n",
    "                initializer=tf.constant_initializer(0), trainable=False),\n",
    "            tf.get_variable(\"link_matrix\", [batch_size, words_num, words_num],\n",
    "                initializer=tf.constant_initializer(0), trainable=False),\n",
    "            tf.get_variable(\"write_weights\", [batch_size, words_num],\n",
    "                initializer=tf.constant_initializer(1e-6), trainable=False),\n",
    "            tf.get_variable(\"read_weights\", [batch_size, words_num, read_heads],\n",
    "                initializer=tf.constant_initializer(1e-6), trainable=False),\n",
    "            tf.get_variable(\"read_vector\", [batch_size, word_size, read_heads],\n",
    "                initializer=tf.constant_initializer(1e-6), trainable=False),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __get_lookup_weighting__(mem_mat, keys, strengths):\n",
    "    normalized_memory = tf.nn.l2_normalize(mem_mat, 2)\n",
    "    normalized_keys = tf.nn.l2_normalize(keys, 1)\n",
    "    \n",
    "    correlation = tf.matmul(normalized_memory, normalized_keys)\n",
    "    strengths = tf.expand_dims(strengths, 1)\n",
    "    \n",
    "    return tf.nn.softmax(correlation * strengths, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __update_usage_vector__(usage_vec, read_weightings, write_weighting, free_gates):\n",
    "    free_gates = tf.expand_dims(free_gates, 1)\n",
    "    \n",
    "    retention_vector = tf.reduce_prod(1 - read_weightings * free_gates, 2)\n",
    "    updated_usage = (usage_vec + write_weighting - usage_vec * write_weighting) * retention_vector\n",
    "    \n",
    "    return updated_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __get_allocation_weighting__(sorted_usage, free_list):\n",
    "    shifted_cumprod = tf.cumprod(sorted_usage, axis = 1, exclusive=True)\n",
    "    unordered_allocation_weighting = (1 - sorted_usage) * shifted_cumprod\n",
    "\n",
    "    mapped_free_list = free_list + index_mapper\n",
    "    flat_unordered_allocation_weighting = tf.reshape(unordered_allocation_weighting, (-1,))\n",
    "    flat_mapped_free_list = tf.reshape(mapped_free_list, (-1,))\n",
    "    flat_container = tf.TensorArray(tf.float32, batch_size *words_num)\n",
    "\n",
    "    flat_ordered_weightings = flat_container.scatter(\n",
    "        flat_mapped_free_list,\n",
    "        flat_unordered_allocation_weighting\n",
    "    )\n",
    "\n",
    "    packed_wightings = flat_ordered_weightings.pack()\n",
    "    return tf.reshape(packed_wightings, (batch_size, words_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __update_write_weighting__(lookup_weighting, alloc_weighting, write_gate, alloc_gate):\n",
    "    lookup_weighting = tf.squeeze(lookup_weighting)\n",
    "    \n",
    "    updated_write_weighting = write_gate * (alloc_gate * alloc_weighting + (1 - alloc_gate) * lookup_weighting)\n",
    "    \n",
    "    return updated_write_weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __update_memory__(mem_mat, write_weighting, write_vec, erase_vec):\n",
    "    write_weighting = tf.expand_dims(write_weighting, 2)\n",
    "    write_vec = tf.expand_dims(write_vec, 1)\n",
    "    erase_vec = tf.expand_dims(erase_vec, 1)\n",
    "    \n",
    "    erasing = mem_mat * (1 - tf.matmul(write_weighting, erase_vec))\n",
    "    writing = tf.matmul(write_weighting, write_vec)\n",
    "    updated_memory = erasing + writing\n",
    "    \n",
    "    return updated_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __update_precedence_vector__(last_precedence, write_weighting):\n",
    "    reset_factor = 1 - tf.reduce_sum(write_weighting, 1, keep_dims=True)\n",
    "    updated_precedence = reset_factor * last_precedence + write_weighting\n",
    "    \n",
    "    return updated_precedence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __update_link_matrix__(precedence_vec, link_matrix, write_weighting):\n",
    "    write_weighting = tf.expand_dims(write_weighting, 2)\n",
    "    precedence_vec = tf.expand_dims(precedence_vec, 1)\n",
    "    \n",
    "    reset_factor = 1 - utils.pairwise_add(write_weighting, is_batch=True)\n",
    "    updated_link_matrix = reset_factor * link_matrix + tf.matmul(write_weighting, precedence_vec)\n",
    "    updated_link_matrix = (1 - IMat) * updated_link_matrix\n",
    "    \n",
    "    return updated_link_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __get_directional_weightings__(read_weightings, link_matrix):\n",
    "    forward_weighting = tf.matmul(link_matrix, read_weightings)\n",
    "    backward_weighting = tf.batch_matmul(link_matrix, read_weightings, adj_x=True)\n",
    "    \n",
    "    return forward_weighting, backward_weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __update_read_weightings__(lookup_weightings, forward_weighting, backward_weighting, read_mode):\n",
    "    backward_mode = tf.expand_dims(read_mode[:, 0, :], 1) * backward_weighting\n",
    "    lookup_mode = tf.expand_dims(read_mode[:, 1, :], 1) * lookup_weightings\n",
    "    forward_mode = tf.expand_dims(read_mode[:, 2, :], 1) * forward_weighting\n",
    "    updated_read_weightings = backward_mode + lookup_mode + forward_mode\n",
    "\n",
    "    return updated_read_weightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __update_read_vectors__(mem_mat, read_weightings):\n",
    "    updated_read_vectors = tf.batch_matmul(mem_mat, read_weightings, adj_x=True)\n",
    "    return updated_read_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_mem(mem_mat, usage_vector, read_weightings, write_weighting, precedence_vector,\n",
    "             link_matrix, key, strength, free_gates, alloc_gate, write_gate, write_vec, erase_vec):\n",
    "    \n",
    "    lookup_weighting = __get_lookup_weighting__(mem_mat, key, strength)\n",
    "    new_usage_vector = __update_usage_vector__(usage_vector, read_weightings, write_weighting, free_gates)\n",
    "\n",
    "    sorted_usage, free_list = tf.nn.top_k(-1 * new_usage_vector, words_num)\n",
    "    sorted_usage = -1 * sorted_usage\n",
    "\n",
    "    alloc_weighting = __get_allocation_weighting__(sorted_usage, free_list)\n",
    "    new_write_weighting = __update_write_weighting__(lookup_weighting, alloc_weighting, write_gate, alloc_gate)\n",
    "    new_memory_matrix = __update_memory__(mem_mat, new_write_weighting, write_vec, erase_vec)\n",
    "    new_link_matrix = __update_link_matrix__(precedence_vector, link_matrix, new_write_weighting)\n",
    "    new_precedence_vector = __update_precedence_vector__(precedence_vector, new_write_weighting)\n",
    "\n",
    "    return new_usage_vector, new_write_weighting, new_memory_matrix, new_link_matrix, new_precedence_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_mem(memory_matrix, read_weightings, keys, strengths, link_matrix, read_modes):\n",
    "    lookup_weighting = __get_lookup_weighting__(memory_matrix, keys, strengths)\n",
    "    forward_weighting, backward_weighting = __get_directional_weightings__(read_weightings, link_matrix)\n",
    "    new_read_weightings = __update_read_weightings__(lookup_weighting, forward_weighting, backward_weighting, read_modes)\n",
    "    new_read_vectors = __update_read_vectors__(memory_matrix, new_read_weightings)\n",
    "\n",
    "    return new_read_weightings, new_read_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph_step(cur_inp, memory_bank, controller_state, cell, time):\n",
    "    \n",
    "    last_read = memory_bank[6]\n",
    "    cell_output, cell_state, interface = DNC_controller_exe_query(\n",
    "        cur_inp, controller_state, last_read, cell)\n",
    "    \n",
    "    mem_mat, usage_vec, pred_vec, link_mat, wrt_wght, rd_wght, rd_vec = memory_bank\n",
    "    \n",
    "    usage_vec, wrt_wght, mem_mat, link_mat, pred_vec = write_mem(\n",
    "    mem_mat, usage_vec, rd_wght, wrt_wght, pred_vec, link_mat,\n",
    "    interface['write_key'], interface['write_strength'], interface['free_gates'],\n",
    "    interface['allocation_gate'], interface['write_gate'], interface['write_vector'],\n",
    "    interface['erase_vector'])\n",
    "    \n",
    "    rd_wght, rd_vec = read_mem(\n",
    "    mem_mat, rd_wght, interface['read_keys'], interface['read_strengths'],\n",
    "    link_mat, interface['read_modes'])\n",
    "    \n",
    "    unit_output = DNC_controller_wb_output(cell_output, rd_vec)\n",
    "    \n",
    "    return (\n",
    "        (mem_mat, usage_vec, pred_vec, link_mat, wrt_wght, rd_wght, rd_vec),\n",
    "        interface['free_gates'], interface['allocation_gate'], interface['write_gate'],\n",
    "        rd_wght, wrt_wght, usage_vec, unit_output, cell_state, time + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_graph(stories, query, decoder_inputs, loss_labels, meta, keep_prob, prob_list):\n",
    "    \n",
    "    with tf.variable_scope(\"embedding\"):\n",
    "        embedding_weights = tf.get_variable(\n",
    "            \"embedding\", [meta[\"vocab_size\"], word_emb_size])\n",
    "    \n",
    "    embedded_stories = tf.nn.embedding_lookup(embedding_weights, stories)\n",
    "    embedded_query = tf.nn.embedding_lookup(embedding_weights, query)\n",
    "    \n",
    "    sentence_emb = []\n",
    "        \n",
    "    # sentence embedding\n",
    "    with tf.variable_scope(\"sentence_encoder\") as scope:\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "            tf.nn.rnn_cell.LSTMCell(word_emb_size, use_peepholes=True),\n",
    "            output_keep_prob=keep_prob)\n",
    "        \n",
    "        init_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        output_mat = tf.get_variable(\"output_mat\", [word_emb_size, emb_size])\n",
    "        \n",
    "        for sub_id in range(meta['max_story_length']):\n",
    "            if sub_id > 0: scope.reuse_variables()\n",
    "            sen_emb_out, sen_emb_state = tf.nn.dynamic_rnn(\n",
    "                cell, embedded_stories[:, sub_id, :, :], \n",
    "                sequence_length=[meta[\"max_sentence_length\"]] * batch_size, \n",
    "                initial_state=init_state)\n",
    "            \n",
    "            sentence_emb.append(\n",
    "                tf.matmul(tf.squeeze(sen_emb_out[:, -1, :]), output_mat))\n",
    "        \n",
    "        scope.reuse_variables()\n",
    "            \n",
    "        query_emb_out, query_emb_state = tf.nn.dynamic_rnn(\n",
    "                cell, embedded_query, \n",
    "                sequence_length=[meta[\"max_query_length\"]] * batch_size, \n",
    "                initial_state=init_state)\n",
    "        \n",
    "        query_emb = tf.matmul(tf.squeeze(query_emb_out[:, -1, :]), output_mat)\n",
    "\n",
    "\n",
    "    DNC_out = []\n",
    "        \n",
    "    with tf.variable_scope(\"DNC\") as scope:\n",
    "        \n",
    "        controller_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "            tf.nn.rnn_cell.LSTMCell(emb_size, use_peepholes=True),\n",
    "            output_keep_prob=keep_prob)\n",
    "        \n",
    "        # encode stories\n",
    "        for time in range(meta[\"max_story_length\"]):\n",
    "            if time == 0:\n",
    "                memory_bank = __get_memory__()\n",
    "                controller_state = controller_cell.zero_state(\n",
    "                    batch_size, dtype=tf.float32)\n",
    "            else:\n",
    "                scope.reuse_variables()\n",
    "            \n",
    "            cur_inp = sentence_emb[time]\n",
    "\n",
    "            DNC_out.append(\n",
    "                graph_step(\n",
    "                    cur_inp, memory_bank, controller_state, \n",
    "                    controller_cell, time))\n",
    "            \n",
    "            memory_bank = DNC_out[-1][0]\n",
    "            controller_state = DNC_out[-1][-2]\n",
    "         \n",
    "        # encode query\n",
    "        scope.reuse_variables()\n",
    "        query_out = graph_step(query_emb, memory_bank, controller_state, controller_cell, time)\n",
    "        \n",
    "        \n",
    "    # decoder answers\n",
    "    with tf.variable_scope(\"decoder\") as scope:\n",
    "        \n",
    "        decoder_cell = tf.nn.rnn_cell.OutputProjectionWrapper(\n",
    "            tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(\n",
    "                    word_emb_size, use_peepholes=True),\n",
    "                output_keep_prob=keep_prob),\n",
    "            meta[\"vocab_size\"])\n",
    "            \n",
    "\n",
    "        proj_to_c = tf.get_variable(\"proj_to_c\", [output_size, word_emb_size])\n",
    "        proj_to_h = tf.get_variable(\"proj_to_h\", [output_size, word_emb_size])\n",
    "\n",
    "        init_state = tf.nn.rnn_cell.LSTMStateTuple(\n",
    "            tf.matmul(query_out[-3], proj_to_c),\n",
    "            tf.matmul(query_out[-3], proj_to_h))\n",
    "        \n",
    "        prob_list.append(decoder_inputs)\n",
    "        prob_list.append(init_state)\n",
    "        \n",
    "        decode_outputs, decode_states = tf.nn.seq2seq.embedding_rnn_decoder(\n",
    "            decoder_inputs, init_state, decoder_cell, meta[\"vocab_size\"], word_emb_size,\n",
    "            output_projection=None, feed_previous=False, update_embedding_for_previous=True,\n",
    "            scope=None)\n",
    "                \n",
    "        scope.reuse_variables()\n",
    "                \n",
    "        decode_outputs_with_exposure, decode_states_with_exposure = tf.nn.seq2seq.embedding_rnn_decoder(\n",
    "            decoder_inputs, init_state, decoder_cell, meta[\"vocab_size\"], word_emb_size,\n",
    "            output_projection=None, feed_previous=True, update_embedding_for_previous=True,\n",
    "            scope=None)\n",
    "\n",
    "    prob_list.append(decode_outputs)    \n",
    "    \n",
    "    loss_weights = [ \n",
    "        tf.ones_like([batch_size], dtype=tf.float32)\n",
    "    ] * meta[\"max_ans_length\"]\n",
    "    loss = tf.nn.seq2seq.sequence_loss(\n",
    "        decode_outputs, loss_labels, loss_weights)\n",
    "    train_op = tf.train.AdamOptimizer(\n",
    "        learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    test_output = tf.argmax(decode_outputs_with_exposure, axis=2)\n",
    "            \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    return train_op, loss, test_output, merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_placeholders(meta):\n",
    "    return {\n",
    "        \n",
    "        # story shape: [batch, story_length, sentence_length]\n",
    "        \"stories\": tf.placeholder(tf.int32,\n",
    "            shape=[batch_size, meta['max_story_length'], meta['max_sentence_length']]),\n",
    "        \n",
    "        # query shape: [batch, query_length]\n",
    "        \"query\": tf.placeholder(tf.int32,\n",
    "            shape=[batch_size, meta['max_query_length']]),\n",
    "        \n",
    "        # decoder input shape: [answer_length][batch]\n",
    "        \"decoder_inputs\": [\n",
    "            tf.zeros_like(tf.zeros([batch_size]), dtype=tf.int32, name=\"GO\") \n",
    "        ] + [\n",
    "            tf.placeholder(tf.int32, shape=[batch_size], name=\"decoder_inputs_{}\".format(t))\n",
    "            for t in range(meta[\"max_ans_length\"] - 1)\n",
    "        ],\n",
    "        \n",
    "        # loss_labels shape: [answer_length][batch]\n",
    "        \"loss_labels\": [\n",
    "            tf.placeholder(tf.int32, shape=[batch_size], name=\"loss_label_{}\".format(t))\n",
    "            for t in range(meta[\"max_ans_length\"])\n",
    "        ],\n",
    "        \n",
    "        # keep_prob: keep probability for LSTM cell dropout wrappers\n",
    "        \"keep_prob\": tf.placeholder(tf.float32)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_feeds(ph_dict, dataset, train_or_test=\"train\"):\n",
    "    batch = dataset.next_batch(train_or_test)\n",
    "    meta = dataset.metadata\n",
    "    story = np.concatenate(batch[:, 0].tolist(), axis=0).reshape(batch_size, meta['max_story_length'], meta['max_sentence_length'])\n",
    "    query = np.concatenate(batch[:, 1].tolist(), axis=0).reshape(batch_size, -1)\n",
    "    ans = [ np.squeeze(t) for t in np.split(np.transpose(np.concatenate(\n",
    "        batch[:, 2].tolist(), axis=0).reshape(batch_size, -1)), dataset.metadata[\"max_ans_length\"], axis=0)]\n",
    "    dec_inp = ans[:-1]\n",
    "    los_lbl = ans\n",
    "    #sup = np.concatenate(batch[:, 3].tolist(), axis=0).reshape(batch_size, -1)\n",
    "    keep_prob = 0.8 if train_or_test==\"train\" else 1.0\n",
    "    \n",
    "    feed_dict = {\n",
    "        ph_dict[\"stories\"]: story,\n",
    "        ph_dict[\"query\"]: query,\n",
    "        ph_dict[\"keep_prob\"]: keep_prob\n",
    "    }\n",
    "    \n",
    "    for time_step in range(len(los_lbl)):\n",
    "        if time_step < len(los_lbl) - 1:\n",
    "            feed_dict[ph_dict['decoder_inputs'][time_step + 1]] = ans[time_step]\n",
    "        feed_dict[ph_dict['loss_labels'][time_step]] = ans[time_step]\n",
    "        \n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph reseted\n",
      "(163, 64)\n",
      "10432\n",
      "(64, 256)\n",
      "16384\n",
      "(128, 256)\n",
      "32768\n",
      "(256,)\n",
      "256\n",
      "(64,)\n",
      "64\n",
      "(64,)\n",
      "64\n",
      "(64,)\n",
      "64\n",
      "(512, 256)\n",
      "131072\n",
      "(512, 1024)\n",
      "524288\n",
      "(1024,)\n",
      "1024\n",
      "(256,)\n",
      "256\n",
      "(256,)\n",
      "256\n",
      "(256,)\n",
      "256\n",
      "(256, 471)\n",
      "120576\n",
      "(256, 256)\n",
      "65536\n",
      "(256, 256)\n",
      "65536\n",
      "(256, 64)\n",
      "16384\n",
      "(256, 64)\n",
      "16384\n",
      "(163, 64)\n",
      "10432\n",
      "(128, 256)\n",
      "32768\n",
      "(256,)\n",
      "256\n",
      "(64,)\n",
      "64\n",
      "(64,)\n",
      "64\n",
      "(64,)\n",
      "64\n",
      "(64, 163)\n",
      "10432\n",
      "(163,)\n",
      "163\n",
      "1055843\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c94044cabb27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"graph built\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_counter = 1\n",
    "train_loss_curve = []\n",
    "train_loss_curve_by_cat = {}\n",
    "test_loss_curve = []\n",
    "test_loss_curve_by_cat = {}\n",
    "\n",
    "#filename = 'qa6_yes-no-questions'\n",
    "\n",
    "for filename in filenames:    \n",
    "    \n",
    "    dataset = Dataset(DataPath + filename, batch_size)\n",
    "    dataset.load_dataset()\n",
    "    meta = dataset.metadata\n",
    "    \n",
    "    prob_list = []\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    print(\"graph reseted\")\n",
    "    \n",
    "    with graph.as_default():\n",
    "               \n",
    "        # define constants\n",
    "        index_mapper = tf.constant(\n",
    "            np.cumsum([0] + [words_num] * (batch_size - 1), dtype=np.int32)[:, np.newaxis])\n",
    "\n",
    "        IMat = tf.constant(np.identity(words_num, dtype=np.float32))\n",
    "    \n",
    "        # define placeholders\n",
    "        placeholders = define_placeholders(meta)\n",
    "    \n",
    "        # define graph\n",
    "        train_op, loss, test_output, merged = build_graph(\n",
    "            placeholders['stories'],\n",
    "            placeholders['query'],\n",
    "            placeholders['decoder_inputs'],\n",
    "            placeholders['loss_labels'],\n",
    "            dataset.metadata,\n",
    "            placeholders['keep_prob'],\n",
    "            prob_list)\n",
    "        \n",
    "        saver = tf.train.Saver(tf.trainable_variables())\n",
    "        \n",
    "        total_parameters = 0\n",
    "        for variable in tf.trainable_variables():\n",
    "            # shape is an array of tf.Dimension\n",
    "            shape = variable.get_shape()\n",
    "            print(shape)\n",
    "            variable_parametes = 1\n",
    "            for dim in shape:\n",
    "                variable_parametes *= dim.value\n",
    "            print(variable_parametes)\n",
    "            total_parameters += variable_parametes\n",
    "        print(total_parameters)\n",
    "        \n",
    "        raise KeyboardInterrupt\n",
    "        \n",
    "        print(\"graph built\")\n",
    "    \n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "        \n",
    "        with tf.Session(graph=graph, config=config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "            #if not filename == 'qa1_single-supporting-fact':\n",
    "            saver.restore(sess, \"./chkpts/DNC_bAbI_full.ckpt\")\n",
    "            epoch_counter = np.load(\"./counter.npy\")[0]\n",
    "\n",
    "            train_writer = tf.summary.FileWriter(\n",
    "                SummariesDir + '/train')\n",
    "            test_writer = tf.summary.FileWriter(\n",
    "                SummariesDir + '/test')\n",
    "                \n",
    "            print(\"parameters loaded\")\n",
    "            \n",
    "            # training loop\n",
    "            for epoch in range(epoches):\n",
    "                \n",
    "                feed_dict = build_feeds(placeholders, dataset, \"train\")\n",
    "                _, loss_val, merged_val = sess.run([train_op, loss, merged], feed_dict=feed_dict)\n",
    "                \n",
    "                train_loss_curve.append(loss_val)\n",
    "                \n",
    "                train_writer.add_summary(merged_val, epoch_counter)\n",
    "                \n",
    "                epoch_counter += 1\n",
    "                \n",
    "                if epoch % 20==0:\n",
    "                    \n",
    "                    feed_dict = build_feeds(placeholders, dataset, \"test\")\n",
    "                    \n",
    "                    loss_val, merged_val = sess.run([loss, merged], feed_dict=feed_dict)\n",
    "                    \n",
    "                    test_loss_curve.append(loss_val)\n",
    "                    test_writer.add_summary(merged_val, epoch_counter)\n",
    "                \n",
    "                    print(test_loss_curve[-1])\n",
    "                \n",
    "                \n",
    "            print(loss_val)   \n",
    "            saver.save(sess, \"./chkpts/DNC_bAbI_full.ckpt\")\n",
    "            \n",
    "            np.save(\"./{}_train_curve_full_1.npy\".format(filename), np.array(train_loss_curve))\n",
    "            np.save(\"./{}_test_curve_full_1.npy\".format(filename), np.array(test_loss_curve))\n",
    "            train_loss_curve = []\n",
    "            test_loss_curve = []\n",
    "            np.save(\"./counter.npy\", np.array([epoch_counter]))\n",
    "            \n",
    "            print(\"parameter saved\")  \n",
    "            print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    print(shape)\n",
    "    variable_parametes = 1\n",
    "    for dim in shape:\n",
    "        print(dim)\n",
    "        variable_parametes *= dim.value\n",
    "    print(variable_parametes)\n",
    "    total_parameters += variable_parametes\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
